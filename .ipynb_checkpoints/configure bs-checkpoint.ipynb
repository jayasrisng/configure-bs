{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0362e6-3ad4-4af4-8b48-c9f7abb12601",
   "metadata": {},
   "source": [
    "Install xror package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e34daae-5f88-446e-824a-751648b9bf95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/MetaGuard/xror.git#egg=xror"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fab910-d42d-407a-8d0d-c319b2ac6197",
   "metadata": {},
   "source": [
    "Import necessary libraries and set up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99302a61-a157-426f-8207-908645278a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from xror import XROR  # Ensure xror module is available in the notebook environment\n",
    "\n",
    "logging.basicConfig(filename= 'xror_parsing_errors.log', level=logging.INFO, format= '%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the column names based on the number of sensors/measurements\n",
    "frame_columns = [\n",
    "    \"spawnTime\", \"saberSpeed\", \"saberDirX\", \"saberDirY\", \"saberDirZ\",\n",
    "    \"cutDirDeviation\", \"cutPointX\", \"cutPointY\", \"cutPointZ\",\n",
    "    \"cutNormalX\", \"cutNormalY\", \"cutNormalZ\", \"cutDistanceToCenter\",\n",
    "    \"cutAngle\", \"beforeCutRating\", \"afterCutRating\", \"noteID\",\n",
    "    \"speedOK\", \"directionOK\", \"saberTypeOK\", \"wasCutTooSoon\", \"saberType\"\n",
    "]\n",
    "\n",
    "# List of root folders containing subdirectories with XROR files\n",
    "root_folders = ['chunk1', 'chunk2', 'chunk3']\n",
    "\n",
    "# Check if CSV files already exist\n",
    "def csv_files_exist(root_folders):\n",
    "    for root_folder in root_folders:\n",
    "        csv_files = glob.glob(os.path.join(root_folder, '**/*.csv'), recursive=True)\n",
    "        if not csv_files:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1638764a-5613-4ea6-b74d-2d0414c53530",
   "metadata": {},
   "source": [
    "Process XROR files if CSV files don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294c113a-29bf-425a-ae3b-31104f050e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_xror_files(root_folder_path):\n",
    "    # Recursively find all .xror files in the root folder and its subdirectories\n",
    "    xror_files = glob.glob(os.path.join(root_folder_path, '**/*.xror'), recursive=True)\n",
    "\n",
    "    # Process each XROR file\n",
    "    for file_path in xror_files:\n",
    "        try:\n",
    "            base_name = os.path.splitext(file_path)[0]\n",
    "            csv_file_path = base_name + '.csv'\n",
    "\n",
    "            if os.path.exists(csv_file_path):\n",
    "                logging.info(f\"CSV file already exists for {file_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            with open(file_path, 'rb') as f:\n",
    "                binary_data = f.read()\n",
    "\n",
    "            xror_data = XROR.unpack(binary_data)\n",
    "\n",
    "            if len(xror_data.data['frames'][0]) != len(frame_columns):\n",
    "                logging.warning(f\"Column mismatch in {file_path}. Expected {len(frame_columns)} columns, found {len(xror_data.data['frames'][0])}.\")\n",
    "                continue\n",
    "\n",
    "            df_frames = pd.DataFrame(xror_data.data['frames'], columns=frame_columns)\n",
    "            df_frames['directionOK'] = df_frames.apply(lambda row: row['saberDirX'] > 0 and row['saberDirY'] > 0, axis=1)\n",
    "            df_frames.to_csv(csv_file_path, index=False)\n",
    "            logging.info(f\"DataFrame saved to CSV successfully for file: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Process files in all specified root folders if CSV files don't exist\n",
    "if not csv_files_exist(root_folders):\n",
    "    for root_folder in root_folders:\n",
    "        process_xror_files(root_folder)\n",
    "else:\n",
    "    print(\"CSV files already exist. Skipping XROR processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7028c0-5751-464e-bb25-edfb745c661d",
   "metadata": {},
   "source": [
    "print the length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ed339b-0e4d-44a0-8b93-f68869c75a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV files created: 241196\n"
     ]
    }
   ],
   "source": [
    "# Verify that the CSV files were created successfully\n",
    "created_csv_files = []\n",
    "for root_folder in root_folders:\n",
    "    csv_files = glob.glob(os.path.join(root_folder, '**/*.csv'), recursive=True)\n",
    "    created_csv_files.extend(csv_files)\n",
    "\n",
    "print(f\"Total CSV files created: {len(created_csv_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be858324-96b9-43de-9b50-341cb41c18cd",
   "metadata": {},
   "source": [
    "Normalize CSV files if not already normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e2b71-d500-4021-95f9-c579278bb04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_csv_files(root_folders):\n",
    "    for root_folder in root_folders:\n",
    "        # Recursively find all .csv files in the root folder and its subdirectories\n",
    "        csv_files = glob.glob(os.path.join(root_folder, '**/*.csv'), recursive=True)\n",
    "        csv_files = [f for f in csv_files if '_normalized.csv' not in f]\n",
    "        \n",
    "        # Process each CSV file\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                # Construct the path for the normalized file\n",
    "                normalized_file_path = file_path.replace('.csv', '_normalized.csv')\n",
    "                \n",
    "                # If normalized file already exists, skip to avoid reprocessing\n",
    "                if os.path.exists(normalized_file_path):\n",
    "                    continue\n",
    "                \n",
    "                # Load the data\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # Normalize the data\n",
    "                scaler = StandardScaler()\n",
    "                numeric_cols = data.select_dtypes(include=['float64', 'int']).columns\n",
    "                data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "                \n",
    "                # Save the normalized data back to disk\n",
    "                data.to_csv(normalized_file_path, index=False)\n",
    "                print(f\"Data normalized and saved successfully for file: {file_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error normalizing file {file_path}: {e}\")\n",
    "\n",
    "# Check if normalization is already done\n",
    "def normalization_done(root_folders):\n",
    "    for root_folder in root_folders:\n",
    "        normalized_files = glob.glob(os.path.join(root_folder, '**/*_normalized.csv'), recursive=True)\n",
    "        if not normalized_files:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "if not normalization_done(root_folders):\n",
    "    normalize_csv_files(root_folders)\n",
    "else:\n",
    "    print(\"CSV files are already normalized. Skipping normalization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f0e9e-ae7b-41e4-b08f-bc07c8bf4991",
   "metadata": {},
   "source": [
    "Print current Dask configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c3f9ff-7ded-41e4-a3ae-25a7665af25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dask configuration:\n",
      "{'temporary-directory': None, 'visualization': {'engine': None}, 'tokenize': {'ensure-deterministic': False}, 'dataframe': {'shuffle-compression': None, 'parquet': {'metadata-task-size-local': 512, 'metadata-task-size-remote': 16}}, 'array': {'svg': {'size': 120}, 'slicing': {'split-large-chunks': None}}, 'optimization': {'fuse': {'active': None, 'ave-width': 1, 'max-width': None, 'max-height': inf, 'max-depth-new-edges': None, 'subgraphs': None, 'rename-keys': True}}}\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "\n",
    "# Print current Dask configuration\n",
    "print(\"Current Dask configuration:\")\n",
    "print(dask.config.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e8f53-5005-4427-8368-fdb83b519c79",
   "metadata": {},
   "source": [
    "Aggregate and feature engineer data if not already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5e7e1-5ebe-4ca3-b75a-6bbc5c31b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 30hr 27m\n",
      "Part 0 processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import os\n",
    "\n",
    "def aggregate_and_feature_engineer(root_folders):\n",
    "    file_patterns = [os.path.join(root, '**/*_normalized.csv') for root in root_folders]\n",
    "    ddf = dd.read_csv(file_patterns, include_path_column=True)\n",
    "    \n",
    "    ddf['user_id'] = ddf['path'].str.extract(r'/([^/]+)/[^/]+\\.csv$')[0]\n",
    "    ddf['chunk_id'] = ddf['path'].str.extract(r'/(chunk\\d+)/')[0]\n",
    "    \n",
    "    ddf = ddf.drop('path', axis=1)\n",
    "    \n",
    "    aggregations = {\n",
    "        'saberSpeed': ['mean', 'std', 'min', 'max'],\n",
    "        'saberDirX': ['mean', 'std', 'min', 'max'],\n",
    "        'saberDirY': ['mean', 'std', 'min', 'max'],\n",
    "        'saberDirZ': ['mean', 'std', 'min', 'max']\n",
    "    }\n",
    "    \n",
    "    grouped_ddf = ddf.groupby(['chunk_id', 'user_id']).agg(aggregations)\n",
    "    grouped_ddf.columns = ['_'.join(col).strip() for col in grouped_ddf.columns.values]\n",
    "\n",
    "    with ProgressBar():\n",
    "        # Save each part after computing\n",
    "        results = []\n",
    "        for i, part in enumerate(grouped_ddf.to_delayed()):\n",
    "            part_df = part.compute()\n",
    "            if not part_df.empty:\n",
    "                part_df.to_csv(f'aggregated_and_featured_data_part_{i}.csv', index=False)\n",
    "                results.append(part_df)\n",
    "                print(f\"Part {i} processed and saved.\")\n",
    "\n",
    "            # Optionally save a checkpoint for every 10000 parts processed\n",
    "            if i % 10000 == 0 and results:\n",
    "                dd.concat(results).to_csv('aggregated_and_featured_data_checkpoint.csv', index=False)\n",
    "                results = []  # Clear the results list to free up memory\n",
    "\n",
    "        # Final save\n",
    "        if results:\n",
    "            dd.concat(results).to_csv('aggregated_and_featured_data_final.csv', index=False)\n",
    "            print(\"Final data processing complete and saved.\")\n",
    "\n",
    "# Check if aggregation is already done\n",
    "def aggregation_done():\n",
    "    final_file_path = 'aggregated_and_featured_data_final.csv'\n",
    "    return os.path.exists(final_file_path)\n",
    "\n",
    "if not aggregation_done():\n",
    "    aggregate_and_feature_engineer(root_folders)\n",
    "else:\n",
    "    print(\"Data is already aggregated and feature engineered. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f094fe-bd9e-4429-96d5-3d5dc15d3689",
   "metadata": {},
   "source": [
    "Reverse Lookup of a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa0e7a-20eb-4813-af28-a8b458b9135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from dask import delayed, compute\n",
    "from dask.diagnostics import ProgressBar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Function to load normalized data and compute features\n",
    "def compute_features(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    user_id = os.path.basename(file_path).split('_')[0]\n",
    "    chunk_id = os.path.basename(file_path).split('_')[1]\n",
    "    \n",
    "    features = {\n",
    "        'user_id': user_id,\n",
    "        'chunk_id': chunk_id,\n",
    "        'saberSpeed_mean': data['saberSpeed'].mean(),\n",
    "        'saberSpeed_std': data['saberSpeed'].std(),\n",
    "        'saberSpeed_min': data['saberSpeed'].min(),\n",
    "        'saberSpeed_max': data['saberSpeed'].max(),\n",
    "        'saberDirX_mean': data['saberDirX'].mean(),\n",
    "        'saberDirX_std': data['saberDirX'].std(),\n",
    "        'saberDirX_min': data['saberDirX'].min(),\n",
    "        'saberDirX_max': data['saberDirX'].max(),\n",
    "        'saberDirY_mean': data['saberDirY'].mean(),\n",
    "        'saberDirY_std': data['saberDirY'].std(),\n",
    "        'saberDirY_min': data['saberDirY'].min(),\n",
    "        'saberDirY_max': data['saberDirY'].max(),\n",
    "        'saberDirZ_mean': data['saberDirZ'].mean(),\n",
    "        'saberDirZ_std': data['saberDirZ'].std(),\n",
    "        'saberDirZ_min': data['saberDirZ'].min(),\n",
    "        'saberDirZ_max': data['saberDirZ'].max(),\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Function to find the closest match using parallel processing\n",
    "def find_closest_match(given_row, root_folders):\n",
    "    file_paths = []\n",
    "    for root_folder in root_folders:\n",
    "        file_paths.extend(glob.glob(os.path.join(root_folder, '**/*_normalized.csv'), recursive=True))\n",
    "    \n",
    "    # Compute features in parallel\n",
    "    delayed_results = [delayed(compute_features)(file_path) for file_path in file_paths]\n",
    "    \n",
    "    with ProgressBar():\n",
    "        all_features = compute(*delayed_results)\n",
    "    \n",
    "    # Convert all_features to DataFrame\n",
    "    df_features = pd.DataFrame(all_features)\n",
    "    \n",
    "    # Compute the distance between the given row and each user's features\n",
    "    min_distance = float('inf')\n",
    "    closest_user = None\n",
    "    for index, row in df_features.iterrows():\n",
    "        user_features = row.drop(['user_id', 'chunk_id']).values\n",
    "        dist = distance.euclidean(user_features, given_row)\n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            closest_user = row[['user_id', 'chunk_id']]\n",
    "    \n",
    "    return closest_user\n",
    "\n",
    "# Given row values (as an example)\n",
    "given_row = [\n",
    "    -8.631126502990014e-17, 1.000000700190627, -11.797579026170633, 4.126330121837754,\n",
    "    1.0614630720503478e-16, 1.000000700190627, -111.94098228264774, 3.6317406424051977,\n",
    "    8.844734305656087e-17, 1.000000700190627, -6.758512157339628, 6.857955228245537,\n",
    "    -3.6887394447039675e-16, 1.000000700190627, -9.91977109738808, 10.866403895736536\n",
    "]\n",
    "\n",
    "# Root folders\n",
    "root_folders = ['chunk1', 'chunk2', 'chunk3']\n",
    "\n",
    "# Find the closest match\n",
    "closest_user = find_closest_match(given_row, root_folders)\n",
    "print(\"The closest user match is:\", closest_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9517fe9-b1fd-4b87-836f-954bf416349f",
   "metadata": {},
   "source": [
    "deep motion masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d4f86-003d-429f-a785-a4677645ce5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Load the data in chunks using Dask\n",
    "file_path = 'aggregated_and_featured_data_final.csv'\n",
    "dask_df = dd.read_csv(file_path)\n",
    "\n",
    "# Normalize function\n",
    "def normalize_chunk(chunk):\n",
    "    scaler = StandardScaler()\n",
    "    chunk_scaled = pd.DataFrame(scaler.fit_transform(chunk), columns=chunk.columns)\n",
    "    return chunk_scaled, scaler\n",
    "\n",
    "# Autoencoder architecture\n",
    "def build_autoencoder(input_dim):\n",
    "    encoding_dim = int(input_dim / 2)\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
    "    decoder = Dense(input_dim, activation=\"sigmoid\")(encoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss=\"mean_squared_error\")\n",
    "    return autoencoder\n",
    "\n",
    "# Train autoencoder on a subset of the data (initial training)\n",
    "sample_df = dask_df.sample(frac=0.1).compute()  # Take a sample of 10% of the data for training\n",
    "features = sample_df.drop(['user_id', 'chunk_id'], axis=1)\n",
    "features_scaled, scaler = normalize_chunk(features)\n",
    "\n",
    "input_dim = features_scaled.shape[1]\n",
    "autoencoder = build_autoencoder(input_dim)\n",
    "autoencoder.fit(features_scaled, features_scaled, epochs=50, batch_size=256, shuffle=True, validation_split=0.2)\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk, autoencoder, scaler, chunk_id):\n",
    "    user_ids = chunk['user_id']\n",
    "    chunk_ids = chunk['chunk_id']\n",
    "    chunk = chunk.drop(['user_id', 'chunk_id'], axis=1)\n",
    "    \n",
    "    chunk_scaled = scaler.transform(chunk)\n",
    "    encoded_data = autoencoder.predict(chunk_scaled)\n",
    "    \n",
    "    masked_chunk = pd.DataFrame(encoded_data, columns=chunk.columns)\n",
    "    masked_chunk['user_id'] = user_ids\n",
    "    masked_chunk['chunk_id'] = chunk_ids\n",
    "    \n",
    "    masked_chunk.to_csv(f'masked_data_chunk_{chunk_id}.csv', index=False)\n",
    "    print(f\"Processed and saved chunk {chunk_id}\")\n",
    "\n",
    "# Process data in chunks\n",
    "def process_data_in_chunks(dask_df, autoencoder, scaler):\n",
    "    with ProgressBar():\n",
    "        chunk_id = 0\n",
    "        for chunk in dask_df.to_delayed():\n",
    "            chunk_df = chunk.compute()\n",
    "            process_chunk(chunk_df, autoencoder, scaler, chunk_id)\n",
    "            chunk_id += 1\n",
    "\n",
    "process_data_in_chunks(dask_df, autoencoder, scaler)\n",
    "print(\"Deep motion masking applied and data saved in chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882e7e9-7eb2-42cc-8793-e143467d3c78",
   "metadata": {},
   "source": [
    "GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35deec33-80e0-49d4-a7cf-c4e72b399d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Generator model\n",
    "def build_generator(input_dim, output_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(output_dim, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# Discriminator model\n",
    "def build_discriminator(input_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(256, input_dim=input_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Combined GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(generator.input_shape[1],))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n",
    "    return gan\n",
    "\n",
    "# Normalize function\n",
    "def normalize_chunk(chunk):\n",
    "    scaler = StandardScaler()\n",
    "    chunk_scaled = pd.DataFrame(scaler.fit_transform(chunk), columns=chunk.columns)\n",
    "    return chunk_scaled, scaler\n",
    "\n",
    "# Load the data in chunks using Dask\n",
    "file_path = 'aggregated_and_featured_data_final.csv'\n",
    "dask_df = dd.read_csv(file_path)\n",
    "\n",
    "# Sample a subset of the data for training\n",
    "sample_df = dask_df.sample(frac=0.1).compute()\n",
    "features = sample_df.drop(['user_id', 'chunk_id'], axis=1)\n",
    "features_scaled, scaler = normalize_chunk(features)\n",
    "\n",
    "input_dim = features_scaled.shape[1]\n",
    "\n",
    "# Build and compile the GAN\n",
    "generator = build_generator(input_dim=input_dim, output_dim=input_dim)\n",
    "discriminator = build_discriminator(input_dim=input_dim)\n",
    "discriminator.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "# Train the GAN\n",
    "epochs = 10000\n",
    "batch_size = 256\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Select a random half batch of data\n",
    "    idx = np.random.randint(0, features_scaled.shape[0], batch_size)\n",
    "    real_data = features_scaled[idx]\n",
    "\n",
    "    # Generate a half batch of new data\n",
    "    noise = np.random.normal(0, 1, (batch_size, input_dim))\n",
    "    generated_data = generator.predict(noise)\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train the generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, input_dim))\n",
    "    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "    # Print the progress\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}%] [G loss: {g_loss}]\")\n",
    "\n",
    "# Function to process each chunk\n",
    "def process_chunk(chunk, generator, scaler, chunk_id):\n",
    "    user_ids = chunk['user_id']\n",
    "    chunk_ids = chunk['chunk_id']\n",
    "    chunk = chunk.drop(['user_id', 'chunk_id'], axis=1)\n",
    "    \n",
    "    chunk_scaled = scaler.transform(chunk)\n",
    "    noise = np.random.normal(0, 1, (chunk_scaled.shape[0], input_dim))\n",
    "    generated_data = generator.predict(noise)\n",
    "    \n",
    "    anonymized_chunk = pd.DataFrame(generated_data, columns=chunk.columns)\n",
    "    anonymized_chunk['user_id'] = user_ids\n",
    "    anonymized_chunk['chunk_id'] = chunk_ids\n",
    "    \n",
    "    anonymized_chunk.to_csv(f'anonymized_data_chunk_{chunk_id}.csv', index=False)\n",
    "    print(f\"Processed and saved chunk {chunk_id}\")\n",
    "\n",
    "# Process data in chunks\n",
    "def process_data_in_chunks(dask_df, generator, scaler):\n",
    "    with ProgressBar():\n",
    "        chunk_id = 0\n",
    "        for chunk in dask_df.to_delayed():\n",
    "            chunk_df = chunk.compute()\n",
    "            process_chunk(chunk_df, generator, scaler, chunk_id)\n",
    "            chunk_id += 1\n",
    "\n",
    "process_data_in_chunks(dask_df, generator, scaler)\n",
    "print(\"GAN-based anonymization applied and data saved in chunks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
