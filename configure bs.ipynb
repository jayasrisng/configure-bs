{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fa4833-44bf-45d0-b496-36a2e1c2e56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xror\n",
      "  Cloning https://github.com/MetaGuard/xror.git to /tmp/pip-install-bcdstqwa/xror_9b00d4a304474e7b92b240e797c0649f\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/MetaGuard/xror.git /tmp/pip-install-bcdstqwa/xror_9b00d4a304474e7b92b240e797c0649f\n",
      "  Resolved https://github.com/MetaGuard/xror.git to commit b2177253d97066038ab52bf59714f1966ece903e\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: fpzip in /home/jguthula/anaconda3/lib/python3.10/site-packages (from xror) (1.2.3)\n",
      "Requirement already satisfied: pymongo in /home/jguthula/anaconda3/lib/python3.10/site-packages (from xror) (4.7.2)\n",
      "Requirement already satisfied: numpy in /home/jguthula/anaconda3/lib/python3.10/site-packages (from fpzip->xror) (1.23.5)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/jguthula/anaconda3/lib/python3.10/site-packages (from pymongo->xror) (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/MetaGuard/xror.git#egg=xror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99302a61-a157-426f-8207-908645278a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from xror import XROR  # Ensure xror module is available in the notebook environment\n",
    "\n",
    "logging.basicConfig(filename= 'xror_parsing_errors.log', level=logging.INFO, format= '%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Define the column names based on the number of sensors/measurements\n",
    "frame_columns = [\n",
    "    \"spawnTime\", \"saberSpeed\", \"saberDirX\", \"saberDirY\", \"saberDirZ\",\n",
    "    \"cutDirDeviation\", \"cutPointX\", \"cutPointY\", \"cutPointZ\",\n",
    "    \"cutNormalX\", \"cutNormalY\", \"cutNormalZ\", \"cutDistanceToCenter\",\n",
    "    \"cutAngle\", \"beforeCutRating\", \"afterCutRating\", \"noteID\",\n",
    "    \"speedOK\", \"directionOK\", \"saberTypeOK\", \"wasCutTooSoon\", \"saberType\"\n",
    "]\n",
    "\n",
    "# List of root folders containing subdirectories with XROR files\n",
    "root_folders = ['chunk1', 'chunk2', 'chunk3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294c113a-29bf-425a-ae3b-31104f050e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_xror_files(root_folder_path):\n",
    "    # Recursively find all .xror files in the root folder and its subdirectories\n",
    "    xror_files = glob.glob(os.path.join(root_folder_path, '**/*.xror'), recursive=True)\n",
    "\n",
    "    # Process each XROR file\n",
    "    for file_path in xror_files:\n",
    "        try:\n",
    "            base_name = os.path.splitext(file_path)[0]\n",
    "            csv_file_path = base_name + '.csv'\n",
    "\n",
    "            if os.path.exists(csv_file_path):\n",
    "                logging.info(f\"CSV file already exists for {file_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            with open(file_path, 'rb') as f:\n",
    "                binary_data = f.read()\n",
    "\n",
    "            xror_data = XROR.unpack(binary_data)\n",
    "\n",
    "            if len(xror_data.data['frames'][0]) != len(frame_columns):\n",
    "                logging.warning(f\"Column mismatch in {file_path}. Expected {len(frame_columns)} columns, found {len(xror_data.data['frames'][0])}.\")\n",
    "                continue\n",
    "\n",
    "            df_frames = pd.DataFrame(xror_data.data['frames'], columns=frame_columns)\n",
    "            df_frames['directionOK'] = df_frames.apply(lambda row: row['saberDirX'] > 0 and row['saberDirY'] > 0, axis=1)\n",
    "            df_frames.to_csv(csv_file_path, index=False)\n",
    "            logging.info(f\"DataFrame saved to CSV successfully for file: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Process files in all specified root folders\n",
    "for root_folder in root_folders:\n",
    "    process_xror_files(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ed339b-0e4d-44a0-8b93-f68869c75a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV files created: 241196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/da3b7041-59d3-42ef-88b9-46a224fc9c44_normalized.csv',\n",
       " 'chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/b7cb0530-7a3b-4c38-a87c-8fd5d90ddc03.csv',\n",
       " 'chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/10db8741-e69e-4258-9889-3c156483c3c3.csv',\n",
       " 'chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/8bf7cba6-6c70-4cff-952a-d619817bc604.csv',\n",
       " 'chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/19585113-7981-4b0b-ad0d-833a5cb3f781.csv',\n",
       " 'chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/75bc5ed3-e258-4daf-935f-fcc9d556cf06_normalized.csv',\n",
       " 'chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/d363d14d-5df8-4bb3-b4fb-10aebfcf831b.csv',\n",
       " 'chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/23c2a162-a4ab-46f5-b9a0-1886aef35f5f.csv',\n",
       " 'chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/42cf12a6-16b8-4931-b936-16db720ab32d.csv',\n",
       " 'chunk1/0062dfc1-f071-4690-aeb1-8b1a72fb18a4/ea5c169f-a403-4abc-beec-f8114386f8d8.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that the CSV files were created successfully\n",
    "created_csv_files = []\n",
    "for root_folder in root_folders:\n",
    "    csv_files = glob.glob(os.path.join(root_folder, '**/*.csv'), recursive=True)\n",
    "    created_csv_files.extend(csv_files)\n",
    "\n",
    "print(f\"Total CSV files created: {len(created_csv_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9da7ebc-17da-4f76-adb9-7b7c7609160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def normalize_csv_files(root_folders):\n",
    "    for root_folder in root_folders:\n",
    "        # Recursively find all .csv files in the root folder and its subdirectories\n",
    "        csv_files = glob.glob(os.path.join(root_folder, '**/*.csv'), recursive=True)\n",
    "        csv_files = [f for f in csv_files if '_normalized.csv' not in f]\n",
    "        \n",
    "        # Process each CSV file\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                # Construct the path for the normalized file\n",
    "                normalized_file_path = file_path.replace('.csv', '_normalized.csv')\n",
    "                \n",
    "                # If normalized file already exists, skip to avoid reprocessing\n",
    "                if os.path.exists(normalized_file_path):\n",
    "                    #print(f\"Normalized file already exists for {file_path}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load the data\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # Normalize the data\n",
    "                scaler = StandardScaler()\n",
    "                numeric_cols = data.select_dtypes(include=['float64', 'int']).columns\n",
    "                data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "                \n",
    "                # Save the normalized data back to disk\n",
    "                data.to_csv(normalized_file_path, index=False)\n",
    "                print(f\"Data normalized and saved successfully for file: {file_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error normalizing file {file_path}: {e}\")\n",
    "\n",
    "# Specify the root folder paths where your CSV files are located\n",
    "root_folders = ['chunk1', 'chunk2', 'chunk3']  # Update this path to where your CSV files are stored\n",
    "normalize_csv_files(root_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa5e7e1-5ebe-4ca3-b75a-6bbc5c31b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 33hr 55m\n",
      "Data processed, features engineered, and results saved.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def aggregate_and_feature_engineer(root_folders):\n",
    "    # Create a list to hold all file patterns\n",
    "    file_patterns = [os.path.join(root, '**/*_normalized.csv') for root in root_folders]\n",
    "    \n",
    "    # Read data from all chunks using Dask\n",
    "    ddf = dd.read_csv(file_patterns, include_path_column=True)\n",
    "    \n",
    "    # Extract user and chunk identifiers from the file path\n",
    "    ddf['user_id'] = ddf['path'].apply(lambda x: x.split('/')[2], meta=('x', 'str'))  # Adjust indexing based on your path structure\n",
    "    ddf['chunk_id'] = ddf['path'].apply(lambda x: x.split('/')[1], meta=('x', 'str'))  # Adjust indexing based on your path structure\n",
    "    \n",
    "    # Drop the path column if no longer needed\n",
    "    ddf = ddf.drop('path', axis=1)\n",
    "    \n",
    "    # Perform feature engineering\n",
    "    # Calculate mean, std, min, and max for saberSpeed, saberDirX, saberDirY for each user in each chunk\n",
    "    aggregation = {\n",
    "        'saberSpeed': ['mean', 'std', 'min', 'max'],\n",
    "        'saberDirX': ['mean', 'std', 'min', 'max'],\n",
    "        'saberDirY': ['mean', 'std', 'min', 'max'],\n",
    "    }\n",
    "\n",
    "    # Perform the aggregation per user and chunk\n",
    "    grouped_ddf = ddf.groupby(['chunk_id', 'user_id']).agg(aggregations)\n",
    "\n",
    "    # Flatten the multi-level column index\n",
    "    grouped_ddf.columns = ['_'.join(col) for col in grouped_ddf.columns.values]\n",
    "\n",
    "    # Compute a small sample to print\n",
    "    sample_df = grouped_ddf.head()\n",
    "    print(sample_df)\n",
    "\n",
    "    # Compute and save the results\n",
    "    with ProgressBar():\n",
    "        result_df = grouped_ddf.compute()\n",
    "        result_df.to_csv('aggregated_and_featured_data.csv', index=False)\n",
    "\n",
    "    print(\"Data processing, features engineered, and results saved.\")\n",
    "\n",
    "# Specify the root folder path where your CSV files are located\n",
    "root_folders = ['chunk1', 'chunk2', 'chunk3']\n",
    "aggregate_and_feature_engineer(root_folders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
