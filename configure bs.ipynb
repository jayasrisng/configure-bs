{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fa4833-44bf-45d0-b496-36a2e1c2e56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xror\n",
      "  Cloning https://github.com/MetaGuard/xror.git to /tmp/pip-install-or09pvt9/xror_c7b3aa18edf6481493bf6a9bd94da2b6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/MetaGuard/xror.git /tmp/pip-install-or09pvt9/xror_c7b3aa18edf6481493bf6a9bd94da2b6\n",
      "  Resolved https://github.com/MetaGuard/xror.git to commit b2177253d97066038ab52bf59714f1966ece903e\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: fpzip in /home/jguthula/anaconda3/lib/python3.10/site-packages (from xror) (1.2.3)\n",
      "Requirement already satisfied: pymongo in /home/jguthula/anaconda3/lib/python3.10/site-packages (from xror) (4.7.2)\n",
      "Requirement already satisfied: numpy in /home/jguthula/anaconda3/lib/python3.10/site-packages (from fpzip->xror) (1.23.5)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/jguthula/anaconda3/lib/python3.10/site-packages (from pymongo->xror) (2.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/MetaGuard/xror.git#egg=xror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99302a61-a157-426f-8207-908645278a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from xror import XROR  # Ensure xror module is available in the notebook environment\n",
    "\n",
    "logging.basicConfig(filename= 'xror_parsing_errors.log', level=logging.INFO, format= '%(asctime)s - %(levelname)s - %(message)s')\n",
    "# Define the column names based on the number of sensors/measurements\n",
    "frame_columns = [\n",
    "    \"spawnTime\", \"saberSpeed\", \"saberDirX\", \"saberDirY\", \"saberDirZ\",\n",
    "    \"cutDirDeviation\", \"cutPointX\", \"cutPointY\", \"cutPointZ\",\n",
    "    \"cutNormalX\", \"cutNormalY\", \"cutNormalZ\", \"cutDistanceToCenter\",\n",
    "    \"cutAngle\", \"beforeCutRating\", \"afterCutRating\", \"noteID\",\n",
    "    \"speedOK\", \"directionOK\", \"saberTypeOK\", \"wasCutTooSoon\", \"saberType\"\n",
    "]\n",
    "\n",
    "# List of root folders containing subdirectories with XROR files\n",
    "root_folders = ['chunk1', 'chunk2', 'chunk3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294c113a-29bf-425a-ae3b-31104f050e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_xror_files(root_folder_path):\n",
    "    # Recursively find all .xror files in the root folder and its subdirectories\n",
    "    xror_files = glob.glob(os.path.join(root_folder_path, '**/*.xror'), recursive=True)\n",
    "\n",
    "    # Process each XROR file\n",
    "    for file_path in xror_files:\n",
    "        try:\n",
    "            base_name = os.path.splitext(file_path)[0]\n",
    "            csv_file_path = base_name + '.csv'\n",
    "\n",
    "            if os.path.exists(csv_file_path):\n",
    "                logging.info(f\"CSV file already exists for {file_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            with open(file_path, 'rb') as f:\n",
    "                binary_data = f.read()\n",
    "\n",
    "            xror_data = XROR.unpack(binary_data)\n",
    "\n",
    "            if len(xror_data.data['frames'][0]) != len(frame_columns):\n",
    "                logging.warning(f\"Column mismatch in {file_path}. Expected {len(frame_columns)} columns, found {len(xror_data.data['frames'][0])}.\")\n",
    "                continue\n",
    "\n",
    "            df_frames = pd.DataFrame(xror_data.data['frames'], columns=frame_columns)\n",
    "            df_frames['directionOK'] = df_frames.apply(lambda row: row['saberDirX'] > 0 and row['saberDirY'] > 0, axis=1)\n",
    "            df_frames.to_csv(csv_file_path, index=False)\n",
    "            logging.info(f\"DataFrame saved to CSV successfully for file: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Process files in all specified root folders\n",
    "for root_folder in root_folders:\n",
    "    process_xror_files(root_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ed339b-0e4d-44a0-8b93-f68869c75a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CSV files created: 241196\n"
     ]
    }
   ],
   "source": [
    "# Verify that the CSV files were created successfully\n",
    "created_csv_files = []\n",
    "for root_folder in root_folders:\n",
    "    csv_files = glob.glob(os.path.join(root_folder, '**/*.csv'), recursive=True)\n",
    "    created_csv_files.extend(csv_files)\n",
    "\n",
    "print(f\"Total CSV files created: {len(created_csv_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9da7ebc-17da-4f76-adb9-7b7c7609160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def normalize_csv_files(root_folders):\n",
    "    for root_folder in root_folders:\n",
    "        # Recursively find all .csv files in the root folder and its subdirectories\n",
    "        csv_files = glob.glob(os.path.join(root_folder, '**/*.csv'), recursive=True)\n",
    "        csv_files = [f for f in csv_files if '_normalized.csv' not in f]\n",
    "        \n",
    "        # Process each CSV file\n",
    "        for file_path in csv_files:\n",
    "            try:\n",
    "                # Construct the path for the normalized file\n",
    "                normalized_file_path = file_path.replace('.csv', '_normalized.csv')\n",
    "                \n",
    "                # If normalized file already exists, skip to avoid reprocessing\n",
    "                if os.path.exists(normalized_file_path):\n",
    "                    #print(f\"Normalized file already exists for {file_path}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load the data\n",
    "                data = pd.read_csv(file_path)\n",
    "                \n",
    "                # Normalize the data\n",
    "                scaler = StandardScaler()\n",
    "                numeric_cols = data.select_dtypes(include=['float64', 'int']).columns\n",
    "                data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "                \n",
    "                # Save the normalized data back to disk\n",
    "                data.to_csv(normalized_file_path, index=False)\n",
    "                print(f\"Data normalized and saved successfully for file: {file_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error normalizing file {file_path}: {e}\")\n",
    "\n",
    "# Specify the root folder paths where your CSV files are located\n",
    "root_folders = ['chunk1', 'chunk2', 'chunk3']  # Update this path to where your CSV files are stored\n",
    "normalize_csv_files(root_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7c3f9ff-7ded-41e4-a3ae-25a7665af25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dask configuration:\n",
      "{'temporary-directory': None, 'visualization': {'engine': None}, 'tokenize': {'ensure-deterministic': False}, 'dataframe': {'shuffle-compression': None, 'parquet': {'metadata-task-size-local': 512, 'metadata-task-size-remote': 16}}, 'array': {'svg': {'size': 120}, 'slicing': {'split-large-chunks': None}}, 'optimization': {'fuse': {'active': None, 'ave-width': 1, 'max-width': None, 'max-height': inf, 'max-depth-new-edges': None, 'subgraphs': None, 'rename-keys': True}}}\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "\n",
    "# Print current Dask configuration\n",
    "print(\"Current Dask configuration:\")\n",
    "print(dask.config.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5e7e1-5ebe-4ca3-b75a-6bbc5c31b688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 30hr 27m\n",
      "Part 0 processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import os\n",
    "\n",
    "def aggregate_and_feature_engineer(root_folders):\n",
    "    file_patterns = [os.path.join(root, '**/*_normalized.csv') for root in root_folders]\n",
    "    ddf = dd.read_csv(file_patterns, include_path_column=True)\n",
    "    \n",
    "    ddf['user_id'] = ddf['path'].str.extract(r'/([^/]+)/[^/]+\\.csv$')[0]\n",
    "    ddf['chunk_id'] = ddf['path'].str.extract(r'/(chunk\\d+)/')[0]\n",
    "    \n",
    "    ddf = ddf.drop('path', axis=1)\n",
    "    \n",
    "    aggregations = {\n",
    "        'saberSpeed': ['mean', 'std', 'min', 'max'],\n",
    "        'saberDirX': ['mean', 'std', 'min', 'max'],\n",
    "        'saberDirY': ['mean', 'std', 'min', 'max'],\n",
    "        'saberDirZ': ['mean', 'std', 'min', 'max']\n",
    "    }\n",
    "    \n",
    "    grouped_ddf = ddf.groupby(['chunk_id', 'user_id']).agg(aggregations)\n",
    "    grouped_ddf.columns = ['_'.join(col).strip() for col in grouped_ddf.columns.values]\n",
    "\n",
    "    with ProgressBar():\n",
    "        # Save each part after computing\n",
    "        results = []\n",
    "        for i, part in enumerate(grouped_ddf.to_delayed()):\n",
    "            part_df = part.compute()\n",
    "            if not part_df.empty:\n",
    "                part_df.to_csv(f'aggregated_and_featured_data_part_{i}.csv', index=False)\n",
    "                results.append(part_df)\n",
    "                print(f\"Part {i} processed and saved.\")\n",
    "\n",
    "            # Optionally save a checkpoint for every 10000 parts processed\n",
    "            if i % 10000 == 0 and results:\n",
    "                dd.concat(results).to_csv('aggregated_and_featured_data_checkpoint.csv', index=False)\n",
    "                results = []  # Clear the results list to free up memory\n",
    "\n",
    "        # Final save\n",
    "        if results:\n",
    "            dd.concat(results).to_csv('aggregated_and_featured_data_final.csv', index=False)\n",
    "            print(\"Final data processing complete and saved.\")\n",
    "\n",
    "root_folders = ['chunk1', 'chunk2', 'chunk3']\n",
    "aggregate_and_feature_engineer(root_folders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda_env)",
   "language": "python",
   "name": "conda_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
